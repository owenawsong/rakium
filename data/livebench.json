{
  "source": "livebench.ai (via HuggingFace)",
  "scraped_at": "2026-02-08T04:31:28.391412",
  "categories": {
    "categories_found": [
      "language",
      "coding",
      "instruction_following"
    ],
    "language": {
      "models": [
        {
          "model": "o1-preview-2024-09-12",
          "avg_score": 0.7736,
          "num_evaluations": 190
        },
        {
          "model": "sonar-pro",
          "avg_score": 0.6873,
          "num_evaluations": 140
        },
        {
          "model": "gemini-2.5-pro-exp-03-25",
          "avg_score": 0.6593,
          "num_evaluations": 240
        },
        {
          "model": "o1-2024-12-17-medium",
          "avg_score": 0.6475,
          "num_evaluations": 140
        },
        {
          "model": "o1-2024-12-17-high",
          "avg_score": 0.6354,
          "num_evaluations": 240
        },
        {
          "model": "claude-3-7-sonnet-20250219-thinking-64k",
          "avg_score": 0.6291,
          "num_evaluations": 240
        },
        {
          "model": "gpt-4.5-preview-2025-02-27",
          "avg_score": 0.62,
          "num_evaluations": 240
        },
        {
          "model": "claude-3-7-sonnet-20250219-thinking-25k",
          "avg_score": 0.6141,
          "num_evaluations": 140
        },
        {
          "model": "o1-2024-12-17-low",
          "avg_score": 0.6113,
          "num_evaluations": 140
        },
        {
          "model": "claude-3-7-sonnet-20250219-base",
          "avg_score": 0.609,
          "num_evaluations": 240
        },
        {
          "model": "qwen2.5-max",
          "avg_score": 0.5842,
          "num_evaluations": 240
        },
        {
          "model": "claude-3-5-sonnet-20241022",
          "avg_score": 0.5636,
          "num_evaluations": 290
        },
        {
          "model": "grok-3-beta",
          "avg_score": 0.5592,
          "num_evaluations": 190
        },
        {
          "model": "gemini-exp-1206",
          "avg_score": 0.5519,
          "num_evaluations": 240
        },
        {
          "model": "claude-3-opus-20240229",
          "avg_score": 0.5273,
          "num_evaluations": 290
        },
        {
          "model": "claude-3-5-sonnet-20240620",
          "avg_score": 0.5218,
          "num_evaluations": 190
        },
        {
          "model": "gemini-2.0-flash",
          "avg_score": 0.5208,
          "num_evaluations": 100
        },
        {
          "model": "grok-3",
          "avg_score": 0.5144,
          "num_evaluations": 140
        },
        {
          "model": "chatgpt-4o-latest-2025-03-27",
          "avg_score": 0.5138,
          "num_evaluations": 290
        },
        {
          "model": "gpt-4o-2024-05-13",
          "avg_score": 0.512,
          "num_evaluations": 190
        },
        {
          "model": "gemini-2.0-pro-exp-02-05",
          "avg_score": 0.511,
          "num_evaluations": 240
        },
        {
          "model": "deepseek-r1-local-2",
          "avg_score": 0.5041,
          "num_evaluations": 140
        },
        {
          "model": "gpt-4o-2024-08-06",
          "avg_score": 0.5017,
          "num_evaluations": 290
        },
        {
          "model": "Meta-Llama-3.1-405B-Instruct-Turbo",
          "avg_score": 0.5002,
          "num_evaluations": 140
        },
        {
          "model": "gpt-4-0613",
          "avg_score": 0.4986,
          "num_evaluations": 140
        },
        {
          "model": "o3-mini-2025-01-31-high",
          "avg_score": 0.4946,
          "num_evaluations": 240
        },
        {
          "model": "chatgpt-4o-latest-2025-01-29",
          "avg_score": 0.4941,
          "num_evaluations": 240
        },
        {
          "model": "deepseek-r1",
          "avg_score": 0.4936,
          "num_evaluations": 240
        },
        {
          "model": "gpt-4o-2024-11-20",
          "avg_score": 0.4902,
          "num_evaluations": 290
        },
        {
          "model": "deepseek-v3-0324",
          "avg_score": 0.4869,
          "num_evaluations": 240
        },
        {
          "model": "gpt-4-1106-preview",
          "avg_score": 0.4858,
          "num_evaluations": 140
        },
        {
          "model": "grok-2-1212",
          "avg_score": 0.4833,
          "num_evaluations": 240
        },
        {
          "model": "chatgpt-4o-latest-2025-01-30",
          "avg_score": 0.4824,
          "num_evaluations": 140
        },
        {
          "model": "hunyuan-turbos-20250313",
          "avg_score": 0.4772,
          "num_evaluations": 240
        },
        {
          "model": "qwq-32b",
          "avg_score": 0.4769,
          "num_evaluations": 240
        },
        {
          "model": "gemini-1.5-pro-exp-0801",
          "avg_score": 0.4732,
          "num_evaluations": 140
        },
        {
          "model": "llama-4-maverick-17b-128e-instruct",
          "avg_score": 0.4722,
          "num_evaluations": 240
        },
        {
          "model": "meta-llama-3.1-405b-instruct-turbo",
          "avg_score": 0.4722,
          "num_evaluations": 240
        },
        {
          "model": "grok-beta",
          "avg_score": 0.4699,
          "num_evaluations": 240
        },
        {
          "model": "deepseek-v3",
          "avg_score": 0.4656,
          "num_evaluations": 240
        },
        {
          "model": "deepseek-r1-local",
          "avg_score": 0.4654,
          "num_evaluations": 37
        },
        {
          "model": "o1-mini-2024-09-12",
          "avg_score": 0.4595,
          "num_evaluations": 290
        },
        {
          "model": "gemini-1.5-pro-exp-0827",
          "avg_score": 0.4533,
          "num_evaluations": 190
        },
        {
          "model": "gemini-2.0-flash-001",
          "avg_score": 0.4517,
          "num_evaluations": 240
        },
        {
          "model": "o3-mini-2025-01-31-medium",
          "avg_score": 0.4498,
          "num_evaluations": 240
        },
        {
          "model": "gemini-2.0-flash-thinking-exp-01-21",
          "avg_score": 0.4463,
          "num_evaluations": 240
        },
        {
          "model": "gpt-4-turbo-2024-04-09",
          "avg_score": 0.4448,
          "num_evaluations": 190
        },
        {
          "model": "step-2-16k-202411",
          "avg_score": 0.4428,
          "num_evaluations": 290
        },
        {
          "model": "hermes-3-llama-3.1-70b",
          "avg_score": 0.4423,
          "num_evaluations": 140
        },
        {
          "model": "gemini-2.0-flash-thinking-exp-1219",
          "avg_score": 0.4356,
          "num_evaluations": 240
        },
        {
          "model": "sonar",
          "avg_score": 0.434,
          "num_evaluations": 190
        },
        {
          "model": "gemini-1.5-pro-002",
          "avg_score": 0.4312,
          "num_evaluations": 290
        },
        {
          "model": "gemini-2.0-flash-exp",
          "avg_score": 0.4304,
          "num_evaluations": 240
        },
        {
          "model": "dracarys2-llama-3.1-70b-instruct",
          "avg_score": 0.4276,
          "num_evaluations": 240
        },
        {
          "model": "llama-3.3-70b-instruct-turbo",
          "avg_score": 0.4263,
          "num_evaluations": 240
        },
        {
          "model": "Meta-Llama-3.1-70B-Instruct-Turbo",
          "avg_score": 0.4257,
          "num_evaluations": 140
        },
        {
          "model": "mistral-large-2411",
          "avg_score": 0.4217,
          "num_evaluations": 240
        },
        {
          "model": "coding-meta-llama-3.1-70b-instruct-chk-50",
          "avg_score": 0.4185,
          "num_evaluations": 140
        },
        {
          "model": "coding2-amcfull-apifull-mmlu12k-meta-llama-3.1-70b-instruct-chk-150",
          "avg_score": 0.4162,
          "num_evaluations": 140
        },
        {
          "model": "learnlm-1.5-pro-experimental",
          "avg_score": 0.4116,
          "num_evaluations": 240
        },
        {
          "model": "amazon.nova-pro-v1:0",
          "avg_score": 0.4109,
          "num_evaluations": 240
        },
        {
          "model": "gemini-exp-1114",
          "avg_score": 0.41,
          "num_evaluations": 190
        },
        {
          "model": "gemma-3-27b-it",
          "avg_score": 0.4039,
          "num_evaluations": 240
        },
        {
          "model": "meta-llama-3.1-70b-instruct-turbo",
          "avg_score": 0.4012,
          "num_evaluations": 240
        },
        {
          "model": "gpt-4-0125-preview",
          "avg_score": 0.4003,
          "num_evaluations": 190
        },
        {
          "model": "qwen2.5-72b-instruct-turbo",
          "avg_score": 0.3978,
          "num_evaluations": 100
        },
        {
          "model": "claude-3-5-haiku-20241022",
          "avg_score": 0.395,
          "num_evaluations": 290
        },
        {
          "model": "o3-mini-2025-01-31-low",
          "avg_score": 0.3945,
          "num_evaluations": 240
        },
        {
          "model": "grok-2-mini",
          "avg_score": 0.3942,
          "num_evaluations": 140
        },
        {
          "model": "gemini-exp-1121",
          "avg_score": 0.3884,
          "num_evaluations": 190
        },
        {
          "model": "claude-3-sonnet-20240229",
          "avg_score": 0.3837,
          "num_evaluations": 140
        },
        {
          "model": "gemini-2.0-flash-lite-preview-02-05",
          "avg_score": 0.3811,
          "num_evaluations": 240
        },
        {
          "model": "grok-3-mini-reasoning-beta",
          "avg_score": 0.3751,
          "num_evaluations": 140
        },
        {
          "model": "codegen3_5k-qwen2.5-72b-instruct-2-chk-50",
          "avg_score": 0.3732,
          "num_evaluations": 140
        },
        {
          "model": "qwen2.5-coder-32b-instruct",
          "avg_score": 0.3699,
          "num_evaluations": 98
        },
        {
          "model": "mistral-large-2407",
          "avg_score": 0.3681,
          "num_evaluations": 190
        },
        {
          "model": "command-a-03-2025",
          "avg_score": 0.367,
          "num_evaluations": 150
        },
        {
          "model": "gemini-2.0-flash-lite-001",
          "avg_score": 0.3557,
          "num_evaluations": 240
        },
        {
          "model": "Qwen2.5-72B-Instruct-Turbo",
          "avg_score": 0.3528,
          "num_evaluations": 190
        },
        {
          "model": "mistral-small-2501",
          "avg_score": 0.3511,
          "num_evaluations": 240
        },
        {
          "model": "grok-2",
          "avg_score": 0.35,
          "num_evaluations": 140
        },
        {
          "model": "gemini-1.5-pro-001",
          "avg_score": 0.3472,
          "num_evaluations": 190
        },
        {
          "model": "dracarys2-72b-instruct",
          "avg_score": 0.347,
          "num_evaluations": 240
        },
        {
          "model": "Meta-Llama-3-70B-Instruct",
          "avg_score": 0.3432,
          "num_evaluations": 140
        },
        {
          "model": "llama-3.1-nemotron-70b-instruct",
          "avg_score": 0.3417,
          "num_evaluations": 140
        },
        {
          "model": "gemma-2-27b-it",
          "avg_score": 0.3404,
          "num_evaluations": 290
        },
        {
          "model": "Reflection-Llama-3.1-70B",
          "avg_score": 0.3392,
          "num_evaluations": 140
        },
        {
          "model": "mistral-small-2503",
          "avg_score": 0.3377,
          "num_evaluations": 240
        },
        {
          "model": "deepseek-coder",
          "avg_score": 0.3317,
          "num_evaluations": 140
        },
        {
          "model": "gpt-4o-mini-2024-07-18",
          "avg_score": 0.329,
          "num_evaluations": 290
        },
        {
          "model": "wbot-4:347b_no_s",
          "avg_score": 0.3252,
          "num_evaluations": 140
        },
        {
          "model": "gemma-3-12b-it",
          "avg_score": 0.3127,
          "num_evaluations": 150
        },
        {
          "model": "command-r-plus-08-2024",
          "avg_score": 0.311,
          "num_evaluations": 290
        },
        {
          "model": "lcb-math-qwen2-72b-instructv3-merged-50",
          "avg_score": 0.3107,
          "num_evaluations": 140
        },
        {
          "model": "deepseek-chat",
          "avg_score": 0.3016,
          "num_evaluations": 190
        },
        {
          "model": "acm_rewrite_qwen2-72B-Chat",
          "avg_score": 0.3003,
          "num_evaluations": 140
        },
        {
          "model": "deepseek-r1-distill-qwen-32b",
          "avg_score": 0.2988,
          "num_evaluations": 240
        },
        {
          "model": "gemini-1.5-flash-exp-0827",
          "avg_score": 0.2957,
          "num_evaluations": 190
        },
        {
          "model": "deepseek-r1-distill-llama-70b",
          "avg_score": 0.2945,
          "num_evaluations": 240
        },
        {
          "model": "phi-4",
          "avg_score": 0.2933,
          "num_evaluations": 150
        },
        {
          "model": "gpt-3.5-turbo-1106",
          "avg_score": 0.2924,
          "num_evaluations": 140
        },
        {
          "model": "Qwen2-72B-Instruct",
          "avg_score": 0.289,
          "num_evaluations": 140
        },
        {
          "model": "amazon.nova-lite-v1:0",
          "avg_score": 0.2864,
          "num_evaluations": 240
        },
        {
          "model": "mistral-large-2402",
          "avg_score": 0.2838,
          "num_evaluations": 140
        },
        {
          "model": "azerogpt",
          "avg_score": 0.2807,
          "num_evaluations": 240
        },
        {
          "model": "gemini-1.5-flash-002",
          "avg_score": 0.2791,
          "num_evaluations": 190
        },
        {
          "model": "gemini-1.5-flash-001",
          "avg_score": 0.2785,
          "num_evaluations": 190
        },
        {
          "model": "mistral-small-2409",
          "avg_score": 0.2778,
          "num_evaluations": 240
        },
        {
          "model": "Dracarys2-72B-Instruct",
          "avg_score": 0.2767,
          "num_evaluations": 50
        },
        {
          "model": "DeepSeek-R1-Distill-Qwen-32B",
          "avg_score": 0.2692,
          "num_evaluations": 140
        },
        {
          "model": "claude-3-haiku-20240307",
          "avg_score": 0.2662,
          "num_evaluations": 190
        },
        {
          "model": "Mixtral-8x22B-Instruct-v0.1",
          "avg_score": 0.2635,
          "num_evaluations": 140
        },
        {
          "model": "Llama-3.1-Nemotron-70B-Instruct-HF",
          "avg_score": 0.2567,
          "num_evaluations": 50
        },
        {
          "model": "gemma-2-9b-it",
          "avg_score": 0.2505,
          "num_evaluations": 190
        },
        {
          "model": "gpt-3.5-turbo-0125",
          "avg_score": 0.2471,
          "num_evaluations": 140
        },
        {
          "model": "open-mixtral-8x22b",
          "avg_score": 0.2459,
          "num_evaluations": 150
        },
        {
          "model": "Qwen2.5-Coder-32B-Instruct",
          "avg_score": 0.2456,
          "num_evaluations": 190
        },
        {
          "model": "QwQ-32B-Preview",
          "avg_score": 0.2363,
          "num_evaluations": 140
        },
        {
          "model": "gemini-1.5-flash-8b-001",
          "avg_score": 0.2287,
          "num_evaluations": 150
        },
        {
          "model": "meta-llama-3.1-8b-instruct-turbo",
          "avg_score": 0.2261,
          "num_evaluations": 240
        },
        {
          "model": "command-r-08-2024",
          "avg_score": 0.2261,
          "num_evaluations": 290
        },
        {
          "model": "amazon.nova-micro-v1:0",
          "avg_score": 0.2201,
          "num_evaluations": 240
        },
        {
          "model": "gemini-1.5-flash-8b-exp-0827",
          "avg_score": 0.2092,
          "num_evaluations": 190
        },
        {
          "model": "qwen2.5-7b-instruct-turbo",
          "avg_score": 0.2082,
          "num_evaluations": 100
        },
        {
          "model": "qwq-32b-preview",
          "avg_score": 0.2051,
          "num_evaluations": 240
        },
        {
          "model": "sky-t1-32b-preview",
          "avg_score": 0.2048,
          "num_evaluations": 140
        },
        {
          "model": "qwen2-math-72b-instruct",
          "avg_score": 0.2044,
          "num_evaluations": 140
        },
        {
          "model": "Meta-Llama-3.1-8B-Instruct-Turbo",
          "avg_score": 0.2027,
          "num_evaluations": 140
        },
        {
          "model": "command-r-plus-04-2024",
          "avg_score": 0.2009,
          "num_evaluations": 190
        },
        {
          "model": "mistral-small-2402",
          "avg_score": 0.1922,
          "num_evaluations": 190
        },
        {
          "model": "Meta-Llama-3-8B-Instruct",
          "avg_score": 0.1893,
          "num_evaluations": 140
        },
        {
          "model": "gemini-1.5-flash-8b-exp-0924",
          "avg_score": 0.1874,
          "num_evaluations": 140
        },
        {
          "model": "Phi-3.5-MoE-instruct",
          "avg_score": 0.1728,
          "num_evaluations": 140
        },
        {
          "model": "Phi-3-small-128k-instruct",
          "avg_score": 0.1566,
          "num_evaluations": 140
        },
        {
          "model": "mathstral-7B-v0.1",
          "avg_score": 0.1541,
          "num_evaluations": 140
        },
        {
          "model": "Phi-3-small-8k-instruct",
          "avg_score": 0.1508,
          "num_evaluations": 140
        },
        {
          "model": "gemma-3-4b-it",
          "avg_score": 0.1506,
          "num_evaluations": 150
        },
        {
          "model": "Qwen2.5-7B-Instruct-Turbo",
          "avg_score": 0.1449,
          "num_evaluations": 190
        },
        {
          "model": "open-mistral-nemo",
          "avg_score": 0.1397,
          "num_evaluations": 140
        },
        {
          "model": "Phi-3-medium-4k-instruct",
          "avg_score": 0.1378,
          "num_evaluations": 140
        },
        {
          "model": "Mixtral-8x7B-Instruct-v0.1",
          "avg_score": 0.1358,
          "num_evaluations": 140
        },
        {
          "model": "Qwen1.5-110B-Chat",
          "avg_score": 0.1307,
          "num_evaluations": 140
        },
        {
          "model": "command-r-03-2024",
          "avg_score": 0.1292,
          "num_evaluations": 190
        },
        {
          "model": "Phi-3-medium-128k-instruct",
          "avg_score": 0.1261,
          "num_evaluations": 140
        },
        {
          "model": "Mistral-7B-Instruct-v0.3",
          "avg_score": 0.116,
          "num_evaluations": 140
        },
        {
          "model": "OpenHermes-2.5-Mistral-7B",
          "avg_score": 0.111,
          "num_evaluations": 140
        },
        {
          "model": "Qwen1.5-72B-Chat",
          "avg_score": 0.109,
          "num_evaluations": 140
        },
        {
          "model": "olmo-2-1124-13b-instruct",
          "avg_score": 0.109,
          "num_evaluations": 140
        },
        {
          "model": "DeepSeek-Coder-V2-Lite-Instruct",
          "avg_score": 0.1058,
          "num_evaluations": 140
        },
        {
          "model": "gemma-1.1-7b-it",
          "avg_score": 0.105,
          "num_evaluations": 140
        },
        {
          "model": "Qwen2-7B-Instruct",
          "avg_score": 0.0989,
          "num_evaluations": 140
        },
        {
          "model": "Phi-3.5-mini-instruct",
          "avg_score": 0.0951,
          "num_evaluations": 140
        },
        {
          "model": "DeepSeek-V2-Lite-Chat",
          "avg_score": 0.0918,
          "num_evaluations": 140
        },
        {
          "model": "Starling-LM-7B-beta",
          "avg_score": 0.0898,
          "num_evaluations": 140
        },
        {
          "model": "Mistral-7B-Instruct-v0.2",
          "avg_score": 0.0877,
          "num_evaluations": 140
        },
        {
          "model": "vicuna-7b-v1.5",
          "avg_score": 0.0849,
          "num_evaluations": 140
        },
        {
          "model": "vicuna-7b-v1.5-16k",
          "avg_score": 0.0771,
          "num_evaluations": 140
        },
        {
          "model": "Phi-3-mini-4k-instruct",
          "avg_score": 0.0769,
          "num_evaluations": 140
        },
        {
          "model": "Phi-3-mini-128k-instruct",
          "avg_score": 0.0751,
          "num_evaluations": 140
        },
        {
          "model": "zephyr-7b-alpha",
          "avg_score": 0.0688,
          "num_evaluations": 140
        },
        {
          "model": "Llama-2-7b-chat-hf",
          "avg_score": 0.0659,
          "num_evaluations": 140
        },
        {
          "model": "phi-3-mini-128k-instruct",
          "avg_score": 0.0617,
          "num_evaluations": 50
        },
        {
          "model": "Qwen1.5-7B-Chat",
          "avg_score": 0.0583,
          "num_evaluations": 140
        },
        {
          "model": "Qwen1.5-4B-Chat",
          "avg_score": 0.0554,
          "num_evaluations": 140
        },
        {
          "model": "phi-3-mini-4k-instruct",
          "avg_score": 0.045,
          "num_evaluations": 50
        },
        {
          "model": "Yi-6B-Chat",
          "avg_score": 0.043,
          "num_evaluations": 140
        },
        {
          "model": "zephyr-7b-beta",
          "avg_score": 0.0382,
          "num_evaluations": 140
        },
        {
          "model": "Qwen1.5-1.8B-Chat",
          "avg_score": 0.027,
          "num_evaluations": 140
        },
        {
          "model": "Qwen2-1.5B-Instruct",
          "avg_score": 0.0261,
          "num_evaluations": 140
        },
        {
          "model": "Qwen1.5-0.5B-Chat",
          "avg_score": 0.0247,
          "num_evaluations": 140
        },
        {
          "model": "Qwen2-0.5B-Instruct",
          "avg_score": 0.024,
          "num_evaluations": 140
        },
        {
          "model": "mistral-large",
          "avg_score": 0.0186,
          "num_evaluations": 100
        },
        {
          "model": "claude-3-5-opus-20240229",
          "avg_score": 0.0142,
          "num_evaluations": 4
        }
      ],
      "num_models": 173
    },
    "coding": {
      "models": [
        {
          "model": "deepseek-r1-local",
          "avg_score": 0.8788,
          "num_evaluations": 33
        },
        {
          "model": "gemini-2.5-pro-exp-03-25",
          "avg_score": 0.8672,
          "num_evaluations": 128
        },
        {
          "model": "o3-mini-2025-01-31-high",
          "avg_score": 0.8281,
          "num_evaluations": 128
        },
        {
          "model": "qwq-32b",
          "avg_score": 0.7578,
          "num_evaluations": 128
        },
        {
          "model": "gpt-4.5-preview-2025-02-27",
          "avg_score": 0.75,
          "num_evaluations": 128
        },
        {
          "model": "claude-3-7-sonnet-20250219-thinking-64k",
          "avg_score": 0.7422,
          "num_evaluations": 128
        },
        {
          "model": "claude-3-7-sonnet-20250219-thinking-25k",
          "avg_score": 0.7188,
          "num_evaluations": 128
        },
        {
          "model": "deepseek-r1-local-2",
          "avg_score": 0.7109,
          "num_evaluations": 128
        },
        {
          "model": "deepseek-v3-0324",
          "avg_score": 0.7109,
          "num_evaluations": 128
        },
        {
          "model": "deepseek-r1",
          "avg_score": 0.7031,
          "num_evaluations": 128
        },
        {
          "model": "o3-mini-2025-01-31-medium",
          "avg_score": 0.6953,
          "num_evaluations": 128
        },
        {
          "model": "o1-2024-12-17-high",
          "avg_score": 0.6875,
          "num_evaluations": 128
        },
        {
          "model": "claude-3-7-sonnet-20250219-base",
          "avg_score": 0.6562,
          "num_evaluations": 128
        },
        {
          "model": "claude-3-5-sonnet-20241022",
          "avg_score": 0.6562,
          "num_evaluations": 128
        },
        {
          "model": "chatgpt-4o-latest-2025-03-27",
          "avg_score": 0.6562,
          "num_evaluations": 128
        },
        {
          "model": "o3-mini-2025-01-31-low",
          "avg_score": 0.6484,
          "num_evaluations": 128
        },
        {
          "model": "qwen2.5-max",
          "avg_score": 0.6406,
          "num_evaluations": 128
        },
        {
          "model": "o1",
          "avg_score": 0.6328,
          "num_evaluations": 128
        },
        {
          "model": "wbot-4:347b_no_s",
          "avg_score": 0.6328,
          "num_evaluations": 128
        },
        {
          "model": "gemini-exp-1206",
          "avg_score": 0.6328,
          "num_evaluations": 128
        },
        {
          "model": "gemini-2.0-pro-exp-02-05",
          "avg_score": 0.625,
          "num_evaluations": 128
        },
        {
          "model": "deepseek-v3",
          "avg_score": 0.6172,
          "num_evaluations": 128
        },
        {
          "model": "claude-3-5-sonnet-20240620",
          "avg_score": 0.6016,
          "num_evaluations": 128
        },
        {
          "model": "sonar-pro",
          "avg_score": 0.6016,
          "num_evaluations": 128
        },
        {
          "model": "chatgpt-4o-latest-2025-01-29",
          "avg_score": 0.5938,
          "num_evaluations": 128
        },
        {
          "model": "grok-3-mini-reasoning-beta",
          "avg_score": 0.5859,
          "num_evaluations": 128
        },
        {
          "model": "dracarys2-72b-instruct",
          "avg_score": 0.5781,
          "num_evaluations": 128
        },
        {
          "model": "Qwen2.5-Coder-32B-Instruct",
          "avg_score": 0.5703,
          "num_evaluations": 128
        },
        {
          "model": "Qwen2.5-72B-Instruct-Turbo",
          "avg_score": 0.5625,
          "num_evaluations": 128
        },
        {
          "model": "deepseek-r1-distill-llama-70b",
          "avg_score": 0.5547,
          "num_evaluations": 128
        },
        {
          "model": "codegen3_5k-qwen2.5-72b-instruct-2-chk-50",
          "avg_score": 0.5547,
          "num_evaluations": 128
        },
        {
          "model": "o1-2024-12-17-medium",
          "avg_score": 0.5547,
          "num_evaluations": 128
        },
        {
          "model": "sky-t1-32b-preview",
          "avg_score": 0.5469,
          "num_evaluations": 128
        },
        {
          "model": "gemini-2.0-flash-thinking-exp-01-21",
          "avg_score": 0.5469,
          "num_evaluations": 128
        },
        {
          "model": "gemini-2.0-flash-thinking-exp-1219",
          "avg_score": 0.5469,
          "num_evaluations": 128
        },
        {
          "model": "gemini-2.0-flash-001",
          "avg_score": 0.5391,
          "num_evaluations": 128
        },
        {
          "model": "grok-3",
          "avg_score": 0.5391,
          "num_evaluations": 128
        },
        {
          "model": "gemini-2.0-flash-exp",
          "avg_score": 0.5312,
          "num_evaluations": 128
        },
        {
          "model": "o1-2024-12-17-low",
          "avg_score": 0.5312,
          "num_evaluations": 128
        },
        {
          "model": "o1-preview-2024-09-12",
          "avg_score": 0.5234,
          "num_evaluations": 128
        },
        {
          "model": "grok-3-beta",
          "avg_score": 0.5234,
          "num_evaluations": 128
        },
        {
          "model": "o1-mini-2024-09-12",
          "avg_score": 0.5156,
          "num_evaluations": 128
        },
        {
          "model": "llama-4-maverick-17b-128e-instruct",
          "avg_score": 0.5111,
          "num_evaluations": 90
        },
        {
          "model": "claude-3-5-haiku-20241022",
          "avg_score": 0.5078,
          "num_evaluations": 128
        },
        {
          "model": "gemini-exp-1114",
          "avg_score": 0.5077,
          "num_evaluations": 130
        },
        {
          "model": "gpt-4o-2024-08-06",
          "avg_score": 0.5,
          "num_evaluations": 128
        },
        {
          "model": "gpt-4o-2024-05-13",
          "avg_score": 0.5,
          "num_evaluations": 128
        },
        {
          "model": "gemini-exp-1121",
          "avg_score": 0.4923,
          "num_evaluations": 130
        },
        {
          "model": "gpt-4-turbo-2024-04-09",
          "avg_score": 0.4922,
          "num_evaluations": 128
        },
        {
          "model": "hunyuan-turbos-20250313",
          "avg_score": 0.4844,
          "num_evaluations": 128
        },
        {
          "model": "chatgpt-4o-latest-2025-01-30",
          "avg_score": 0.4766,
          "num_evaluations": 128
        },
        {
          "model": "gemini-1.5-pro-002",
          "avg_score": 0.4766,
          "num_evaluations": 128
        },
        {
          "model": "gemini-2.0-flash-lite-001",
          "avg_score": 0.4688,
          "num_evaluations": 128
        },
        {
          "model": "mistral-large-2411",
          "avg_score": 0.4688,
          "num_evaluations": 128
        },
        {
          "model": "grok-2-1212",
          "avg_score": 0.4688,
          "num_evaluations": 128
        },
        {
          "model": "mistral-large-2407",
          "avg_score": 0.4688,
          "num_evaluations": 128
        },
        {
          "model": "gpt-4-1106-preview",
          "avg_score": 0.4609,
          "num_evaluations": 128
        },
        {
          "model": "gpt-4o-2024-11-20",
          "avg_score": 0.4609,
          "num_evaluations": 128
        },
        {
          "model": "step-2-16k-202411",
          "avg_score": 0.457,
          "num_evaluations": 256
        },
        {
          "model": "learnlm-1.5-pro-experimental",
          "avg_score": 0.4531,
          "num_evaluations": 128
        },
        {
          "model": "grok-beta",
          "avg_score": 0.4453,
          "num_evaluations": 128
        },
        {
          "model": "deepseek-chat",
          "avg_score": 0.4438,
          "num_evaluations": 178
        },
        {
          "model": "gemini-2.0-flash-lite-preview-02-05",
          "avg_score": 0.4375,
          "num_evaluations": 128
        },
        {
          "model": "Meta-Llama-3.1-405B-Instruct-Turbo",
          "avg_score": 0.4375,
          "num_evaluations": 128
        },
        {
          "model": "gpt-4o-mini-2024-07-18",
          "avg_score": 0.4297,
          "num_evaluations": 128
        },
        {
          "model": "gpt-4-0125-preview",
          "avg_score": 0.4219,
          "num_evaluations": 128
        },
        {
          "model": "QwQ-32B-Preview",
          "avg_score": 0.4141,
          "num_evaluations": 128
        },
        {
          "model": "qwen2-math-72b-instruct",
          "avg_score": 0.4141,
          "num_evaluations": 128
        },
        {
          "model": "qwq-32b-preview",
          "avg_score": 0.4141,
          "num_evaluations": 128
        },
        {
          "model": "deepseek-coder",
          "avg_score": 0.4141,
          "num_evaluations": 128
        },
        {
          "model": "gemini-1.5-flash-002",
          "avg_score": 0.4141,
          "num_evaluations": 128
        },
        {
          "model": "meta-llama-3.1-405b-instruct-turbo",
          "avg_score": 0.4141,
          "num_evaluations": 128
        },
        {
          "model": "gemini-1.5-pro-exp-0801",
          "avg_score": 0.4031,
          "num_evaluations": 129
        },
        {
          "model": "gemini-1.5-pro-exp-0827",
          "avg_score": 0.4015,
          "num_evaluations": 132
        },
        {
          "model": "gemma-3-27b-it",
          "avg_score": 0.3984,
          "num_evaluations": 128
        },
        {
          "model": "deepseek-r1-distill-qwen-32b",
          "avg_score": 0.3984,
          "num_evaluations": 128
        },
        {
          "model": "gemini-1.5-flash-exp-0827",
          "avg_score": 0.3953,
          "num_evaluations": 129
        },
        {
          "model": "amazon.nova-pro-v1:0",
          "avg_score": 0.3906,
          "num_evaluations": 128
        },
        {
          "model": "Qwen2.5-7B-Instruct-Turbo",
          "avg_score": 0.3867,
          "num_evaluations": 256
        },
        {
          "model": "acm_rewrite_qwen2-72B-Chat",
          "avg_score": 0.3864,
          "num_evaluations": 88
        },
        {
          "model": "grok-2-mini",
          "avg_score": 0.3828,
          "num_evaluations": 128
        },
        {
          "model": "lcb-math-qwen2-72b-instructv3-merged-50",
          "avg_score": 0.3828,
          "num_evaluations": 128
        },
        {
          "model": "claude-3-opus-20240229",
          "avg_score": 0.3828,
          "num_evaluations": 128
        },
        {
          "model": "DeepSeek-R1-Distill-Qwen-32B",
          "avg_score": 0.3828,
          "num_evaluations": 128
        },
        {
          "model": "llama-3.3-70b-instruct-turbo",
          "avg_score": 0.3672,
          "num_evaluations": 128
        },
        {
          "model": "mistral-small-2503",
          "avg_score": 0.3672,
          "num_evaluations": 128
        },
        {
          "model": "grok-2",
          "avg_score": 0.3672,
          "num_evaluations": 128
        },
        {
          "model": "gpt-4-0613",
          "avg_score": 0.3672,
          "num_evaluations": 128
        },
        {
          "model": "sonar",
          "avg_score": 0.3672,
          "num_evaluations": 256
        },
        {
          "model": "coding-meta-llama-3.1-70b-instruct-chk-50",
          "avg_score": 0.3594,
          "num_evaluations": 128
        },
        {
          "model": "coding2-amcfull-apifull-mmlu12k-meta-llama-3.1-70b-instruct-chk-150",
          "avg_score": 0.3594,
          "num_evaluations": 128
        },
        {
          "model": "dracarys2-llama-3.1-70b-instruct",
          "avg_score": 0.3594,
          "num_evaluations": 128
        },
        {
          "model": "gemma-2-27b-it",
          "avg_score": 0.3594,
          "num_evaluations": 128
        },
        {
          "model": "mistral-small-2501",
          "avg_score": 0.3516,
          "num_evaluations": 128
        },
        {
          "model": "gemini-1.5-flash-001",
          "avg_score": 0.3438,
          "num_evaluations": 128
        },
        {
          "model": "meta-llama-3.1-70b-instruct-turbo",
          "avg_score": 0.3438,
          "num_evaluations": 128
        },
        {
          "model": "qwen2.5-72b-instruct-turbo",
          "avg_score": 0.3333,
          "num_evaluations": 15
        },
        {
          "model": "qwen2.5-7b-instruct-turbo",
          "avg_score": 0.3333,
          "num_evaluations": 15
        },
        {
          "model": "Mixtral-8x22B-Instruct-v0.1",
          "avg_score": 0.3295,
          "num_evaluations": 88
        },
        {
          "model": "Meta-Llama-3.1-70B-Instruct-Turbo",
          "avg_score": 0.3281,
          "num_evaluations": 128
        },
        {
          "model": "gemini-1.5-pro-001",
          "avg_score": 0.3281,
          "num_evaluations": 128
        },
        {
          "model": "llama-3.1-nemotron-70b-instruct",
          "avg_score": 0.3281,
          "num_evaluations": 128
        },
        {
          "model": "Smaug-Qwen2-72B-Instruct",
          "avg_score": 0.325,
          "num_evaluations": 40
        },
        {
          "model": "Qwen2-72B-Instruct",
          "avg_score": 0.3203,
          "num_evaluations": 128
        },
        {
          "model": "azerogpt",
          "avg_score": 0.3125,
          "num_evaluations": 128
        },
        {
          "model": "phi-4",
          "avg_score": 0.3125,
          "num_evaluations": 128
        },
        {
          "model": "hermes-3-llama-3.1-70b",
          "avg_score": 0.3125,
          "num_evaluations": 128
        },
        {
          "model": "Qwen2-7B-Instruct",
          "avg_score": 0.3047,
          "num_evaluations": 128
        },
        {
          "model": "open-mixtral-8x22b",
          "avg_score": 0.3,
          "num_evaluations": 40
        },
        {
          "model": "gemini-1.5-flash-8b-exp-0924",
          "avg_score": 0.2969,
          "num_evaluations": 128
        },
        {
          "model": "open-mistral-nemo",
          "avg_score": 0.2891,
          "num_evaluations": 128
        },
        {
          "model": "gemini-1.5-flash-8b-exp-0827",
          "avg_score": 0.2891,
          "num_evaluations": 128
        },
        {
          "model": "gpt-3.5-turbo-0125",
          "avg_score": 0.2812,
          "num_evaluations": 128
        },
        {
          "model": "mistral-large-2402",
          "avg_score": 0.2812,
          "num_evaluations": 128
        },
        {
          "model": "claude-3-sonnet-20240229",
          "avg_score": 0.2734,
          "num_evaluations": 128
        },
        {
          "model": "gpt-3.5-turbo-1106",
          "avg_score": 0.2734,
          "num_evaluations": 128
        },
        {
          "model": "amazon.nova-lite-v1:0",
          "avg_score": 0.2734,
          "num_evaluations": 128
        },
        {
          "model": "starling-lm-7b-beta",
          "avg_score": 0.2667,
          "num_evaluations": 15
        },
        {
          "model": "mistral-small-2409",
          "avg_score": 0.2656,
          "num_evaluations": 128
        },
        {
          "model": "DeepSeek-Coder-V2-Lite-Instruct",
          "avg_score": 0.2578,
          "num_evaluations": 128
        },
        {
          "model": "claude-3-haiku-20240307",
          "avg_score": 0.25,
          "num_evaluations": 128
        },
        {
          "model": "Phi-3-small-128k-instruct",
          "avg_score": 0.2476,
          "num_evaluations": 206
        },
        {
          "model": "Meta-Llama-3-70B-Instruct",
          "avg_score": 0.2422,
          "num_evaluations": 128
        },
        {
          "model": "Phi-3.5-MoE-instruct",
          "avg_score": 0.2344,
          "num_evaluations": 128
        },
        {
          "model": "Phi-3-medium-128k-instruct",
          "avg_score": 0.2344,
          "num_evaluations": 128
        },
        {
          "model": "Qwen1.5-72B-Chat",
          "avg_score": 0.2344,
          "num_evaluations": 128
        },
        {
          "model": "gemma-2-9b-it",
          "avg_score": 0.2344,
          "num_evaluations": 128
        },
        {
          "model": "Qwen1.5-110B-Chat",
          "avg_score": 0.2266,
          "num_evaluations": 128
        },
        {
          "model": "command-r-plus-04-2024",
          "avg_score": 0.2188,
          "num_evaluations": 128
        },
        {
          "model": "Phi-3-medium-4k-instruct",
          "avg_score": 0.2188,
          "num_evaluations": 128
        },
        {
          "model": "mistral-small-2402",
          "avg_score": 0.2188,
          "num_evaluations": 128
        },
        {
          "model": "Meta-Llama-3.1-8B-Instruct-Turbo",
          "avg_score": 0.2188,
          "num_evaluations": 128
        },
        {
          "model": "Meta-Llama-3-8B-Instruct",
          "avg_score": 0.2109,
          "num_evaluations": 128
        },
        {
          "model": "amazon.nova-micro-v1:0",
          "avg_score": 0.2109,
          "num_evaluations": 128
        },
        {
          "model": "command-r-plus-08-2024",
          "avg_score": 0.207,
          "num_evaluations": 256
        },
        {
          "model": "Phi-3-small-8k-instruct",
          "avg_score": 0.2039,
          "num_evaluations": 206
        },
        {
          "model": "Starling-LM-7B-beta",
          "avg_score": 0.2031,
          "num_evaluations": 128
        },
        {
          "model": "meta-llama-3.1-8b-instruct-turbo",
          "avg_score": 0.2031,
          "num_evaluations": 128
        },
        {
          "model": "mixtral-8x22b-instruct-v0.1",
          "avg_score": 0.2,
          "num_evaluations": 15
        },
        {
          "model": "phi-3-mini-4k-instruct",
          "avg_score": 0.2,
          "num_evaluations": 15
        },
        {
          "model": "qwen1.5-72b-chat",
          "avg_score": 0.2,
          "num_evaluations": 15
        },
        {
          "model": "reflection-llama-3.1-70b",
          "avg_score": 0.2,
          "num_evaluations": 15
        },
        {
          "model": "meta-llama-3-70b-instruct",
          "avg_score": 0.2,
          "num_evaluations": 15
        },
        {
          "model": "qwen1.5-110b-chat",
          "avg_score": 0.2,
          "num_evaluations": 15
        },
        {
          "model": "meta-llama-3-8b-instruct",
          "avg_score": 0.2,
          "num_evaluations": 15
        },
        {
          "model": "Reflection-Llama-3.1-70B",
          "avg_score": 0.1953,
          "num_evaluations": 128
        },
        {
          "model": "command-r-08-2024",
          "avg_score": 0.1875,
          "num_evaluations": 128
        },
        {
          "model": "Phi-3.5-mini-instruct",
          "avg_score": 0.1719,
          "num_evaluations": 128
        },
        {
          "model": "Phi-3-mini-4k-instruct",
          "avg_score": 0.1719,
          "num_evaluations": 256
        },
        {
          "model": "Phi-3-mini-128k-instruct",
          "avg_score": 0.168,
          "num_evaluations": 256
        },
        {
          "model": "mathstral-7B-v0.1",
          "avg_score": 0.1641,
          "num_evaluations": 128
        },
        {
          "model": "command-r-03-2024",
          "avg_score": 0.1641,
          "num_evaluations": 128
        },
        {
          "model": "Mistral-7B-Instruct-v0.2",
          "avg_score": 0.1562,
          "num_evaluations": 128
        },
        {
          "model": "open-mixtral-8x7b",
          "avg_score": 0.15,
          "num_evaluations": 40
        },
        {
          "model": "OpenHermes-2.5-Mistral-7B",
          "avg_score": 0.1484,
          "num_evaluations": 128
        },
        {
          "model": "zephyr-7b-alpha",
          "avg_score": 0.1406,
          "num_evaluations": 128
        },
        {
          "model": "phi-3-mini-128k-instruct",
          "avg_score": 0.1333,
          "num_evaluations": 15
        },
        {
          "model": "mistral-7b-instruct-v0.2",
          "avg_score": 0.1333,
          "num_evaluations": 15
        },
        {
          "model": "qwen1.5-7b-chat",
          "avg_score": 0.1333,
          "num_evaluations": 15
        },
        {
          "model": "mixtral-8x7b-instruct-v0.1",
          "avg_score": 0.1333,
          "num_evaluations": 15
        },
        {
          "model": "Mixtral-8x7B-Instruct-v0.1",
          "avg_score": 0.125,
          "num_evaluations": 88
        },
        {
          "model": "Mistral-7B-Instruct-v0.3",
          "avg_score": 0.125,
          "num_evaluations": 128
        },
        {
          "model": "gemma-1.1-7b-it",
          "avg_score": 0.1172,
          "num_evaluations": 128
        },
        {
          "model": "olmo-2-1124-13b-instruct",
          "avg_score": 0.1094,
          "num_evaluations": 128
        },
        {
          "model": "Qwen1.5-7B-Chat",
          "avg_score": 0.1016,
          "num_evaluations": 128
        },
        {
          "model": "zephyr-7b-beta",
          "avg_score": 0.0938,
          "num_evaluations": 128
        },
        {
          "model": "DeepSeek-V2-Lite-Chat",
          "avg_score": 0.0781,
          "num_evaluations": 128
        },
        {
          "model": "qwen1.5-4b-chat",
          "avg_score": 0.0667,
          "num_evaluations": 15
        },
        {
          "model": "Qwen1.5-4B-Chat",
          "avg_score": 0.0547,
          "num_evaluations": 128
        },
        {
          "model": "Qwen2-1.5B-Instruct",
          "avg_score": 0.0547,
          "num_evaluations": 128
        },
        {
          "model": "perplexity-sonar-reasoning",
          "avg_score": 0.0263,
          "num_evaluations": 38
        },
        {
          "model": "vicuna-7b-v1.5",
          "avg_score": 0.0234,
          "num_evaluations": 128
        },
        {
          "model": "vicuna-7b-v1.5-16k",
          "avg_score": 0.0234,
          "num_evaluations": 128
        },
        {
          "model": "Llama-2-7b-chat-hf",
          "avg_score": 0.0156,
          "num_evaluations": 128
        },
        {
          "model": "Qwen2-0.5B-Instruct",
          "avg_score": 0.0156,
          "num_evaluations": 128
        },
        {
          "model": "Yi-6B-Chat",
          "avg_score": 0.0156,
          "num_evaluations": 128
        },
        {
          "model": "Qwen1.5-0.5B-Chat",
          "avg_score": 0.0,
          "num_evaluations": 128
        },
        {
          "model": "Qwen1.5-1.8B-Chat",
          "avg_score": 0.0,
          "num_evaluations": 128
        },
        {
          "model": "llama-2-7b-chat-hf",
          "avg_score": 0.0,
          "num_evaluations": 15
        },
        {
          "model": "qwen2-72b-instruct",
          "avg_score": 0.0,
          "num_evaluations": 15
        },
        {
          "model": "qwen1.5-1.8b-chat",
          "avg_score": 0.0,
          "num_evaluations": 15
        },
        {
          "model": "qwen1.5-0.5b-chat",
          "avg_score": 0.0,
          "num_evaluations": 15
        },
        {
          "model": "yi-6b-chat",
          "avg_score": 0.0,
          "num_evaluations": 15
        }
      ],
      "num_models": 183
    },
    "instruction_following": {
      "models": [
        {
          "model": "codegen3_5k-qwen2.5-72b-instruct-2-chk-50",
          "avg_score": 0.9215,
          "num_evaluations": 26
        },
        {
          "model": "step-2-16k-202411",
          "avg_score": 0.8771,
          "num_evaluations": 75
        },
        {
          "model": "coding-meta-llama-3.1-70b-instruct-chk-50",
          "avg_score": 0.8686,
          "num_evaluations": 26
        },
        {
          "model": "deepseek-v3-0324",
          "avg_score": 0.8678,
          "num_evaluations": 50
        },
        {
          "model": "Meta-Llama-3.1-70B-Instruct-Turbo",
          "avg_score": 0.867,
          "num_evaluations": 26
        },
        {
          "model": "deepseek-r1-local-2",
          "avg_score": 0.8572,
          "num_evaluations": 50
        },
        {
          "model": "claude-3-7-sonnet-20250219-thinking-64k",
          "avg_score": 0.8568,
          "num_evaluations": 50
        },
        {
          "model": "gemini-2.0-flash-001",
          "avg_score": 0.8553,
          "num_evaluations": 50
        },
        {
          "model": "Qwen2-72B-Instruct",
          "avg_score": 0.8542,
          "num_evaluations": 26
        },
        {
          "model": "qwq-32b",
          "avg_score": 0.853,
          "num_evaluations": 50
        },
        {
          "model": "lcb-math-qwen2-72b-instructv3-merged-50",
          "avg_score": 0.8517,
          "num_evaluations": 25
        },
        {
          "model": "gemini-exp-1121",
          "avg_score": 0.8462,
          "num_evaluations": 76
        },
        {
          "model": "gemini-2.0-flash-lite-preview-02-05",
          "avg_score": 0.842,
          "num_evaluations": 50
        },
        {
          "model": "coding2-amcfull-apifull-mmlu12k-meta-llama-3.1-70b-instruct-chk-150",
          "avg_score": 0.8397,
          "num_evaluations": 26
        },
        {
          "model": "hunyuan-turbos-20250313",
          "avg_score": 0.832,
          "num_evaluations": 50
        },
        {
          "model": "gemini-exp-1206",
          "avg_score": 0.832,
          "num_evaluations": 50
        },
        {
          "model": "o1-preview-2024-09-12",
          "avg_score": 0.8317,
          "num_evaluations": 75
        },
        {
          "model": "claude-3-7-sonnet-20250219-thinking-25k",
          "avg_score": 0.8315,
          "num_evaluations": 50
        },
        {
          "model": "o1-2024-12-17-low",
          "avg_score": 0.8315,
          "num_evaluations": 50
        },
        {
          "model": "o1-2024-12-17-high",
          "avg_score": 0.829,
          "num_evaluations": 50
        },
        {
          "model": "gemini-1.5-flash-002",
          "avg_score": 0.8277,
          "num_evaluations": 76
        },
        {
          "model": "gemini-2.0-pro-exp-02-05",
          "avg_score": 0.8275,
          "num_evaluations": 50
        },
        {
          "model": "o3-mini-2025-01-31-high",
          "avg_score": 0.825,
          "num_evaluations": 50
        },
        {
          "model": "gemini-exp-1114",
          "avg_score": 0.8186,
          "num_evaluations": 76
        },
        {
          "model": "gemini-2.0-flash-thinking-exp-01-21",
          "avg_score": 0.8168,
          "num_evaluations": 50
        },
        {
          "model": "gemini-2.5-pro-exp-03-25",
          "avg_score": 0.8095,
          "num_evaluations": 50
        },
        {
          "model": "o1-2024-12-17-medium",
          "avg_score": 0.8087,
          "num_evaluations": 50
        },
        {
          "model": "grok-2-mini",
          "avg_score": 0.8067,
          "num_evaluations": 25
        },
        {
          "model": "deepseek-r1",
          "avg_score": 0.806,
          "num_evaluations": 50
        },
        {
          "model": "gemini-2.0-flash-exp",
          "avg_score": 0.805,
          "num_evaluations": 50
        },
        {
          "model": "o3-mini-2025-01-31-medium",
          "avg_score": 0.7985,
          "num_evaluations": 50
        },
        {
          "model": "llama-4-maverick-17b-128e-instruct",
          "avg_score": 0.7958,
          "num_evaluations": 50
        },
        {
          "model": "sonar",
          "avg_score": 0.7931,
          "num_evaluations": 75
        },
        {
          "model": "Meta-Llama-3-70B-Instruct",
          "avg_score": 0.7901,
          "num_evaluations": 26
        },
        {
          "model": "o3-mini-2025-01-31-low",
          "avg_score": 0.7858,
          "num_evaluations": 50
        },
        {
          "model": "gemini-2.0-flash-lite-001",
          "avg_score": 0.7845,
          "num_evaluations": 50
        },
        {
          "model": "gemini-1.5-pro-exp-0801",
          "avg_score": 0.7821,
          "num_evaluations": 26
        },
        {
          "model": "llama-3.3-70b-instruct-turbo",
          "avg_score": 0.782,
          "num_evaluations": 50
        },
        {
          "model": "gemini-1.5-flash-exp-0827",
          "avg_score": 0.7798,
          "num_evaluations": 76
        },
        {
          "model": "claude-3-7-sonnet-20250219-base",
          "avg_score": 0.7788,
          "num_evaluations": 50
        },
        {
          "model": "acm_rewrite_qwen2-72B-Chat",
          "avg_score": 0.7772,
          "num_evaluations": 26
        },
        {
          "model": "Meta-Llama-3.1-405B-Instruct-Turbo",
          "avg_score": 0.7772,
          "num_evaluations": 26
        },
        {
          "model": "gpt-4-1106-preview",
          "avg_score": 0.7708,
          "num_evaluations": 26
        },
        {
          "model": "meta-llama-3.1-405b-instruct-turbo",
          "avg_score": 0.7682,
          "num_evaluations": 50
        },
        {
          "model": "qwen2.5-max",
          "avg_score": 0.7663,
          "num_evaluations": 50
        },
        {
          "model": "gemini-1.5-pro-002",
          "avg_score": 0.7629,
          "num_evaluations": 76
        },
        {
          "model": "gemini-2.0-flash-thinking-exp-1219",
          "avg_score": 0.7592,
          "num_evaluations": 50
        },
        {
          "model": "gemini-1.5-flash-8b-exp-0827",
          "avg_score": 0.7581,
          "num_evaluations": 76
        },
        {
          "model": "deepseek-coder",
          "avg_score": 0.758,
          "num_evaluations": 26
        },
        {
          "model": "chatgpt-4o-latest-2025-03-27",
          "avg_score": 0.7564,
          "num_evaluations": 76
        },
        {
          "model": "deepseek-v3",
          "avg_score": 0.7552,
          "num_evaluations": 50
        },
        {
          "model": "grok-3-beta",
          "avg_score": 0.7491,
          "num_evaluations": 75
        },
        {
          "model": "gpt-3.5-turbo-0125",
          "avg_score": 0.7452,
          "num_evaluations": 26
        },
        {
          "model": "Qwen2.5-72B-Instruct-Turbo",
          "avg_score": 0.7441,
          "num_evaluations": 76
        },
        {
          "model": "Phi-3-small-128k-instruct",
          "avg_score": 0.742,
          "num_evaluations": 26
        },
        {
          "model": "Mixtral-8x22B-Instruct-v0.1",
          "avg_score": 0.742,
          "num_evaluations": 26
        },
        {
          "model": "grok-2",
          "avg_score": 0.7367,
          "num_evaluations": 25
        },
        {
          "model": "gemini-1.5-flash-8b-exp-0924",
          "avg_score": 0.7343,
          "num_evaluations": 50
        },
        {
          "model": "Qwen1.5-72B-Chat",
          "avg_score": 0.734,
          "num_evaluations": 26
        },
        {
          "model": "gemini-1.5-pro-exp-0827",
          "avg_score": 0.7315,
          "num_evaluations": 76
        },
        {
          "model": "gpt-4o-2024-08-06",
          "avg_score": 0.7289,
          "num_evaluations": 76
        },
        {
          "model": "meta-llama-3.1-70b-instruct-turbo",
          "avg_score": 0.7277,
          "num_evaluations": 50
        },
        {
          "model": "Phi-3-medium-128k-instruct",
          "avg_score": 0.7276,
          "num_evaluations": 26
        },
        {
          "model": "gpt-4.5-preview-2025-02-27",
          "avg_score": 0.7233,
          "num_evaluations": 50
        },
        {
          "model": "wbot-4:347b_no_s",
          "avg_score": 0.719,
          "num_evaluations": 50
        },
        {
          "model": "gpt-4-0613",
          "avg_score": 0.7179,
          "num_evaluations": 26
        },
        {
          "model": "gemma-3-27b-it",
          "avg_score": 0.715,
          "num_evaluations": 50
        },
        {
          "model": "Phi-3.5-mini-instruct",
          "avg_score": 0.7115,
          "num_evaluations": 26
        },
        {
          "model": "mistral-large-2402",
          "avg_score": 0.71,
          "num_evaluations": 25
        },
        {
          "model": "claude-3-5-sonnet-20240620",
          "avg_score": 0.7062,
          "num_evaluations": 76
        },
        {
          "model": "o1-mini-2024-09-12",
          "avg_score": 0.7057,
          "num_evaluations": 75
        },
        {
          "model": "grok-beta",
          "avg_score": 0.7047,
          "num_evaluations": 50
        },
        {
          "model": "learnlm-1.5-pro-experimental",
          "avg_score": 0.703,
          "num_evaluations": 50
        },
        {
          "model": "deepseek-r1-distill-llama-70b",
          "avg_score": 0.7022,
          "num_evaluations": 50
        },
        {
          "model": "amazon.nova-pro-v1:0",
          "avg_score": 0.7013,
          "num_evaluations": 50
        },
        {
          "model": "gpt-4o-2024-11-20",
          "avg_score": 0.7012,
          "num_evaluations": 76
        },
        {
          "model": "mistral-large-2411",
          "avg_score": 0.6988,
          "num_evaluations": 50
        },
        {
          "model": "llama-3.1-nemotron-70b-instruct",
          "avg_score": 0.6967,
          "num_evaluations": 25
        },
        {
          "model": "deepseek-r1-distill-qwen-32b",
          "avg_score": 0.6962,
          "num_evaluations": 50
        },
        {
          "model": "gpt-4o-2024-05-13",
          "avg_score": 0.6958,
          "num_evaluations": 76
        },
        {
          "model": "Phi-3-small-8k-instruct",
          "avg_score": 0.6923,
          "num_evaluations": 26
        },
        {
          "model": "claude-3-5-sonnet-20241022",
          "avg_score": 0.6917,
          "num_evaluations": 76
        },
        {
          "model": "gemma-2-27b-it",
          "avg_score": 0.6882,
          "num_evaluations": 76
        },
        {
          "model": "gpt-4-turbo-2024-04-09",
          "avg_score": 0.6836,
          "num_evaluations": 76
        },
        {
          "model": "grok-2-1212",
          "avg_score": 0.6785,
          "num_evaluations": 50
        },
        {
          "model": "gemini-1.5-pro-001",
          "avg_score": 0.6698,
          "num_evaluations": 76
        },
        {
          "model": "mistral-large-2407",
          "avg_score": 0.6678,
          "num_evaluations": 75
        },
        {
          "model": "olmo-2-1124-13b-instruct",
          "avg_score": 0.6678,
          "num_evaluations": 50
        },
        {
          "model": "dracarys2-llama-3.1-70b-instruct",
          "avg_score": 0.6672,
          "num_evaluations": 50
        },
        {
          "model": "OpenHermes-2.5-Mistral-7B",
          "avg_score": 0.6635,
          "num_evaluations": 26
        },
        {
          "model": "chatgpt-4o-latest-2025-01-29",
          "avg_score": 0.6612,
          "num_evaluations": 50
        },
        {
          "model": "gpt-4o-mini-2024-07-18",
          "avg_score": 0.658,
          "num_evaluations": 76
        },
        {
          "model": "claude-3-5-haiku-20241022",
          "avg_score": 0.6577,
          "num_evaluations": 76
        },
        {
          "model": "Phi-3-mini-4k-instruct",
          "avg_score": 0.6538,
          "num_evaluations": 26
        },
        {
          "model": "deepseek-chat",
          "avg_score": 0.6515,
          "num_evaluations": 76
        },
        {
          "model": "claude-3-opus-20240229",
          "avg_score": 0.651,
          "num_evaluations": 76
        },
        {
          "model": "mistral-small-2501",
          "avg_score": 0.6507,
          "num_evaluations": 50
        },
        {
          "model": "Qwen2.5-Coder-32B-Instruct",
          "avg_score": 0.6486,
          "num_evaluations": 76
        },
        {
          "model": "gpt-4-0125-preview",
          "avg_score": 0.647,
          "num_evaluations": 76
        },
        {
          "model": "Dracarys2-72B-Instruct",
          "avg_score": 0.645,
          "num_evaluations": 50
        },
        {
          "model": "mistral-small-2402",
          "avg_score": 0.642,
          "num_evaluations": 75
        },
        {
          "model": "chatgpt-4o-latest-2025-01-30",
          "avg_score": 0.6388,
          "num_evaluations": 50
        },
        {
          "model": "Meta-Llama-3-8B-Instruct",
          "avg_score": 0.6378,
          "num_evaluations": 26
        },
        {
          "model": "claude-3-sonnet-20240229",
          "avg_score": 0.6362,
          "num_evaluations": 26
        },
        {
          "model": "command-r-plus-04-2024",
          "avg_score": 0.636,
          "num_evaluations": 76
        },
        {
          "model": "dracarys2-72b-instruct",
          "avg_score": 0.636,
          "num_evaluations": 50
        },
        {
          "model": "vicuna-7b-v1.5-16k",
          "avg_score": 0.6333,
          "num_evaluations": 25
        },
        {
          "model": "command-r-08-2024",
          "avg_score": 0.6286,
          "num_evaluations": 76
        },
        {
          "model": "Mistral-7B-Instruct-v0.2",
          "avg_score": 0.6282,
          "num_evaluations": 26
        },
        {
          "model": "Phi-3.5-MoE-instruct",
          "avg_score": 0.6282,
          "num_evaluations": 26
        },
        {
          "model": "mistral-small-2503",
          "avg_score": 0.6242,
          "num_evaluations": 50
        },
        {
          "model": "grok-3-mini-reasoning-beta",
          "avg_score": 0.6237,
          "num_evaluations": 50
        },
        {
          "model": "open-mistral-nemo",
          "avg_score": 0.6233,
          "num_evaluations": 25
        },
        {
          "model": "azerogpt",
          "avg_score": 0.6105,
          "num_evaluations": 50
        },
        {
          "model": "sonar-pro",
          "avg_score": 0.6102,
          "num_evaluations": 50
        },
        {
          "model": "gemini-1.5-flash-001",
          "avg_score": 0.6072,
          "num_evaluations": 76
        },
        {
          "model": "command-r-plus-08-2024",
          "avg_score": 0.5998,
          "num_evaluations": 76
        },
        {
          "model": "zephyr-7b-beta",
          "avg_score": 0.5883,
          "num_evaluations": 25
        },
        {
          "model": "vicuna-7b-v1.5",
          "avg_score": 0.5817,
          "num_evaluations": 25
        },
        {
          "model": "Qwen2.5-7B-Instruct-Turbo",
          "avg_score": 0.5802,
          "num_evaluations": 76
        },
        {
          "model": "mathstral-7B-v0.1",
          "avg_score": 0.58,
          "num_evaluations": 25
        },
        {
          "model": "Mixtral-8x7B-Instruct-v0.1",
          "avg_score": 0.5785,
          "num_evaluations": 26
        },
        {
          "model": "command-r-03-2024",
          "avg_score": 0.5769,
          "num_evaluations": 26
        },
        {
          "model": "Reflection-Llama-3.1-70B",
          "avg_score": 0.5737,
          "num_evaluations": 26
        },
        {
          "model": "amazon.nova-lite-v1:0",
          "avg_score": 0.571,
          "num_evaluations": 50
        },
        {
          "model": "meta-llama-3.1-8b-instruct-turbo",
          "avg_score": 0.5705,
          "num_evaluations": 50
        },
        {
          "model": "DeepSeek-V2-Lite-Chat",
          "avg_score": 0.5673,
          "num_evaluations": 26
        },
        {
          "model": "gemma-2-9b-it",
          "avg_score": 0.5662,
          "num_evaluations": 76
        },
        {
          "model": "zephyr-7b-alpha",
          "avg_score": 0.5633,
          "num_evaluations": 25
        },
        {
          "model": "DeepSeek-Coder-V2-Lite-Instruct",
          "avg_score": 0.5577,
          "num_evaluations": 26
        },
        {
          "model": "claude-3-haiku-20240307",
          "avg_score": 0.557,
          "num_evaluations": 76
        },
        {
          "model": "Qwen1.5-110B-Chat",
          "avg_score": 0.5513,
          "num_evaluations": 26
        },
        {
          "model": "Phi-3-mini-128k-instruct",
          "avg_score": 0.5481,
          "num_evaluations": 26
        },
        {
          "model": "phi-4",
          "avg_score": 0.548,
          "num_evaluations": 50
        },
        {
          "model": "qwen2-math-72b-instruct",
          "avg_score": 0.5433,
          "num_evaluations": 25
        },
        {
          "model": "Meta-Llama-3.1-8B-Instruct-Turbo",
          "avg_score": 0.5385,
          "num_evaluations": 26
        },
        {
          "model": "Phi-3-medium-4k-instruct",
          "avg_score": 0.5353,
          "num_evaluations": 26
        },
        {
          "model": "gemma-1.1-7b-it",
          "avg_score": 0.5337,
          "num_evaluations": 26
        },
        {
          "model": "Mistral-7B-Instruct-v0.3",
          "avg_score": 0.5304,
          "num_evaluations": 26
        },
        {
          "model": "Llama-3.1-Nemotron-70B-Instruct-HF",
          "avg_score": 0.5285,
          "num_evaluations": 50
        },
        {
          "model": "sky-t1-32b-preview",
          "avg_score": 0.5253,
          "num_evaluations": 50
        },
        {
          "model": "open-mixtral-8x22b",
          "avg_score": 0.5168,
          "num_evaluations": 50
        },
        {
          "model": "hermes-3-llama-3.1-70b",
          "avg_score": 0.515,
          "num_evaluations": 25
        },
        {
          "model": "gpt-3.5-turbo-1106",
          "avg_score": 0.5128,
          "num_evaluations": 26
        },
        {
          "model": "Qwen1.5-7B-Chat",
          "avg_score": 0.4904,
          "num_evaluations": 26
        },
        {
          "model": "Yi-6B-Chat",
          "avg_score": 0.4631,
          "num_evaluations": 26
        },
        {
          "model": "Qwen2-7B-Instruct",
          "avg_score": 0.4615,
          "num_evaluations": 26
        },
        {
          "model": "mistral-small-2409",
          "avg_score": 0.4612,
          "num_evaluations": 50
        },
        {
          "model": "Llama-2-7b-chat-hf",
          "avg_score": 0.4503,
          "num_evaluations": 26
        },
        {
          "model": "phi-3-small-8k-instruct",
          "avg_score": 0.4417,
          "num_evaluations": 50
        },
        {
          "model": "Qwen2-1.5B-Instruct",
          "avg_score": 0.4151,
          "num_evaluations": 26
        },
        {
          "model": "amazon.nova-micro-v1:0",
          "avg_score": 0.412,
          "num_evaluations": 50
        },
        {
          "model": "Starling-LM-7B-beta",
          "avg_score": 0.4119,
          "num_evaluations": 26
        },
        {
          "model": "phi-3-mini-128k-instruct",
          "avg_score": 0.407,
          "num_evaluations": 50
        },
        {
          "model": "phi-3-medium-4k-instruct",
          "avg_score": 0.3953,
          "num_evaluations": 50
        },
        {
          "model": "qwq-32b-preview",
          "avg_score": 0.3678,
          "num_evaluations": 50
        },
        {
          "model": "QwQ-32B-Preview",
          "avg_score": 0.3678,
          "num_evaluations": 50
        },
        {
          "model": "Qwen1.5-4B-Chat",
          "avg_score": 0.3622,
          "num_evaluations": 26
        },
        {
          "model": "DeepSeek-R1-Distill-Qwen-32B",
          "avg_score": 0.3482,
          "num_evaluations": 50
        },
        {
          "model": "Qwen2-0.5B-Instruct",
          "avg_score": 0.3429,
          "num_evaluations": 26
        },
        {
          "model": "phi-3-mini-4k-instruct",
          "avg_score": 0.3427,
          "num_evaluations": 50
        },
        {
          "model": "Qwen1.5-1.8B-Chat",
          "avg_score": 0.2997,
          "num_evaluations": 26
        },
        {
          "model": "Qwen1.5-0.5B-Chat",
          "avg_score": 0.2644,
          "num_evaluations": 26
        }
      ],
      "num_models": 163
    },
    "overall": {
      "models": [
        {
          "avg_score": 0.7409,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.7032,
          "num_evaluations": 393
        },
        {
          "avg_score": 0.691,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.6904,
          "num_evaluations": 318
        },
        {
          "avg_score": 0.6745,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.6722,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.6603,
          "num_evaluations": 70
        },
        {
          "avg_score": 0.6438,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.6429,
          "num_evaluations": 318
        },
        {
          "avg_score": 0.6407,
          "num_evaluations": 318
        },
        {
          "avg_score": 0.6362,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.6355,
          "num_evaluations": 318
        },
        {
          "avg_score": 0.6328,
          "num_evaluations": 128
        },
        {
          "avg_score": 0.6233,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.6137,
          "num_evaluations": 318
        },
        {
          "avg_score": 0.6102,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.6079,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.6073,
          "num_evaluations": 494
        },
        {
          "avg_score": 0.6011,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5951,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.588,
          "num_evaluations": 494
        },
        {
          "avg_score": 0.5838,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5838,
          "num_evaluations": 393
        },
        {
          "avg_score": 0.5833,
          "num_evaluations": 394
        },
        {
          "avg_score": 0.5667,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5467,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5446,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5436,
          "num_evaluations": 394
        },
        {
          "avg_score": 0.5362,
          "num_evaluations": 494
        },
        {
          "avg_score": 0.5267,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5262,
          "num_evaluations": 268
        },
        {
          "avg_score": 0.524,
          "num_evaluations": 380
        },
        {
          "avg_score": 0.5219,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5214,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5208,
          "num_evaluations": 100
        },
        {
          "avg_score": 0.5205,
          "num_evaluations": 396
        },
        {
          "avg_score": 0.5191,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5151,
          "num_evaluations": 494
        },
        {
          "avg_score": 0.5115,
          "num_evaluations": 493
        },
        {
          "avg_score": 0.5109,
          "num_evaluations": 318
        },
        {
          "avg_score": 0.5104,
          "num_evaluations": 396
        },
        {
          "avg_score": 0.5089,
          "num_evaluations": 494
        },
        {
          "avg_score": 0.5084,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5063,
          "num_evaluations": 394
        },
        {
          "avg_score": 0.5061,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5047,
          "num_evaluations": 318
        },
        {
          "avg_score": 0.5022,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.5011,
          "num_evaluations": 621
        },
        {
          "avg_score": 0.5007,
          "num_evaluations": 294
        },
        {
          "avg_score": 0.5002,
          "num_evaluations": 294
        },
        {
          "avg_score": 0.499,
          "num_evaluations": 318
        },
        {
          "avg_score": 0.4974,
          "num_evaluations": 294
        },
        {
          "avg_score": 0.4964,
          "num_evaluations": 394
        },
        {
          "avg_score": 0.494,
          "num_evaluations": 494
        },
        {
          "avg_score": 0.4905,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4898,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4892,
          "num_evaluations": 398
        },
        {
          "avg_score": 0.4698,
          "num_evaluations": 295
        },
        {
          "avg_score": 0.4693,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4646,
          "num_evaluations": 494
        },
        {
          "avg_score": 0.4608,
          "num_evaluations": 294
        },
        {
          "avg_score": 0.4608,
          "num_evaluations": 100
        },
        {
          "avg_score": 0.4592,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4581,
          "num_evaluations": 393
        },
        {
          "avg_score": 0.4549,
          "num_evaluations": 394
        },
        {
          "avg_score": 0.4535,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4529,
          "num_evaluations": 521
        },
        {
          "avg_score": 0.4523,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4508,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4417,
          "num_evaluations": 50
        },
        {
          "avg_score": 0.4416,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4395,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4395,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4353,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4326,
          "num_evaluations": 294
        },
        {
          "avg_score": 0.4289,
          "num_evaluations": 294
        },
        {
          "avg_score": 0.4288,
          "num_evaluations": 394
        },
        {
          "avg_score": 0.4288,
          "num_evaluations": 394
        },
        {
          "avg_score": 0.4244,
          "num_evaluations": 293
        },
        {
          "avg_score": 0.4229,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4227,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.4223,
          "num_evaluations": 294
        },
        {
          "avg_score": 0.4214,
          "num_evaluations": 395
        },
        {
          "avg_score": 0.4185,
          "num_evaluations": 444
        },
        {
          "avg_score": 0.4057,
          "num_evaluations": 494
        },
        {
          "avg_score": 0.4053,
          "num_evaluations": 294
        },
        {
          "avg_score": 0.4032,
          "num_evaluations": 394
        },
        {
          "avg_score": 0.3988,
          "num_evaluations": 494
        },
        {
          "avg_score": 0.3953,
          "num_evaluations": 50
        },
        {
          "avg_score": 0.3929,
          "num_evaluations": 318
        },
        {
          "avg_score": 0.3926,
          "num_evaluations": 100
        },
        {
          "avg_score": 0.3918,
          "num_evaluations": 293
        },
        {
          "avg_score": 0.3905,
          "num_evaluations": 293
        },
        {
          "avg_score": 0.3894,
          "num_evaluations": 115
        },
        {
          "avg_score": 0.3884,
          "num_evaluations": 293
        },
        {
          "avg_score": 0.3871,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.381,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.3789,
          "num_evaluations": 254
        },
        {
          "avg_score": 0.3768,
          "num_evaluations": 418
        },
        {
          "avg_score": 0.3699,
          "num_evaluations": 98
        }
      ],
      "num_models": 195
    }
  }
}